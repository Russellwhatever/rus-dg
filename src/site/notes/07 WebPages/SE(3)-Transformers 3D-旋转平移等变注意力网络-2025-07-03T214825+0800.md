---
{"dg-publish":true,"permalink":"/07-web-pages/se-3-transformers-3-d-2025-07-03-t214825-0800/","title":"SE(3)-Transformers 3D-旋转平移等变注意力网络","tags":["AI人工智能/SE3"],"noteIcon":"","created":"2025-07-03","updated":"2025-07-03T21:58"}
---

#clipping #
from: https://zhuanlan.zhihu.com/p/659891698
author: [[啦啦啦啦啦啦啦啦啦\|啦啦啦啦啦啦啦啦啦]]
published: 
Failed to fetch

![SE(3)-Transformers 3D-旋转平移等变注意力网络](https://picx.zhimg.com/70/v2-6b7c74358878f4f0ae5603a28bef24a3_1440w.avis?source=172ae18b&biz_tag=Post)

SE(3)-Transformers 3D-旋转平移等变注意力网络

**论文：SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks** <sup><a href="https://zhuanlan.zhihu.com/p/#ref_1">[1]</a></sup>

### 0\. 摘要：

提出一种 SE(3)的-Transformer 机制，是一种3D点云和图形的自注意力机制的变体，能够在3D旋转平移的过程中保持等变。等变性对于输入数据有扰动情况下可以保证预测的稳定性。等变性的一个推论是能够增加模型内部权重的紧密-绑定的性质。在一个玩具的N 体模拟数据集来评估模型，展示模型的预测的鲁棒性。又在两个真实的数据集上进行进行评测 [ScanObjectNN](https://zhida.zhihu.com/search?content_id=234770982&content_type=Article&match_order=1&q=ScanObjectNN&zhida_source=entity) & [QM9](https://zhida.zhihu.com/search?content_id=234770982&content_type=Article&match_order=1&q=QM9&zhida_source=entity). 所有的结果上显示模型的性能都比非-等变的没有attention机制的baseline模型要强。

### 1\. 导论：

通用性的方法对于具体的任务而言，有些底层的知识可能没有被利用。提出SE(3)-Transformer ，针对3D点云和图形数据的自注意力机制，强调 **等变的约束** ，提高在干扰转化和通用性能方面的鲁棒性。

点云数据无处不在，例如3D对象扫描、3D分子结构、N体粒子模拟。 发现自然结构采取合适的点的数目作为输入，以及尊重不规则的点云采样都是具有挑战的问题。此外，一个重要的性质是这些结构 应该对整体输入姿态的全局变化保持不变。意味着3D的平移和旋转变化对于输入的点云结果是不变的。 **发现在自注意力机制中显示的增加等变性约束可以解决这些问题。**

  

![](https://pic2.zhimg.com/v2-b2ec756737cb0dafb968a315a69e94b5_1440w.jpg)

Figure 1: SE(3)-Transformer

自注意力机制本身是一系列点集合的伪线性映射。可以认为有两个部分组成：

- **input-dependent attention weights** 依赖输入的注意力权重
- **embedding of input 输入的嵌入表示** （称为 value embedding）

例如图1中的分子图，每一个原子都有一个嵌入式向量表达 ***value embedding***, ***attention weights*** 对应表示边，边的宽度表示 ***attention weights*** 值的大小。SE(3)-Transformer中显示的设计 ***attention weights*** 对于全局的姿态保持不变。进一步，设计 ***value embedding*** 对于全局的姿态保持不变。等变性的约束约束了学习函数的空间到一个对称的子空间中，降低了需要学习的参数数量。同时提供了丰富的不变性，因为相对的位置信息的特征都得到了保留。

相关的工作有tensor field networks(TFN), 以及其他体素化的3D可操作CNN.主要贡献：

- 提出一个新的自注意力机制，保证输入的全局旋转平移不变性。
- 解决了concurrent SE(3)-equivariant受到角度约束的问题。
- 基于Pytorch 实现了球谐函数，相比Scipy在CPU上快10x倍。100-1000xGPU.\[Code available at [github.com/FabianFuchsM](https://link.zhihu.com/?target=https%3A//github.com/FabianFuchsML/se3-transformer-public)\]

### 2\. 背景

主要介绍自注意力机制、图神经网络以及等变性的相关背景。

#### **2.1 注意力机制**

标准的注意力机制由三个部分组成： 向量 $qi∈Rp，i=1,..,mq_{i} \in R^{p}，i=1,..,mq_{i} \in R^{p}，i=1,..,m$ 的一系列组合， $kj∈Rp,j=1,...,nk_{j} \in R^{p} , j = 1,...,n$ 的一系列组合， $vj∈Rrj=1,...,nv_{j}\in R^{r} j = 1,...,n$ 的组合。r,p是低维向量的向量维度。 $kj和vjk_{j}和v_{j}$ 是对某一点j绑定的联系的表示。对于给定的query $qiq_{i}$ 注意力机制可以写为：

$Attn(qi,{kj},{vj})=∑j=1nαijvj,αij=exp⁡(qiTkj)∑j′=1nexp⁡(qiTkj′)Attn(q_{i},\{k_{j}\},\{v_{j}\}) = \sum_{j=1}^{n} \alpha_{ij}v_{j}  ,\quad \alpha_{ij} = \frac{\exp(q_{i}^{T}k_{j})}{\sum_{j^{'}=1}^{n} \exp(q_{i}^{T}k_{j^{'}})}Attn(q_{i},\{k_{j}\},\{v_{j}\}) = \sum_{j=1}^{n} \alpha_{ij}v_{j}  ,\quad \alpha_{ij} = \frac{\exp(q_{i}^{T}k_{j})}{\sum_{j^{'}=1}^{n} \exp(q_{i}^{T}k_{j^{'}})}$

其中通过softmax作为对权重中的非线性变化。一般来说，query vectors的数量不需要等于输入点云的数量。自注意力机制写为,f是输入的特征：

$q=hQ(f),k=hK(f),v=hV(f)q = h_{Q}(\bold{f}) , k = h_{K}(\bold{f}) , v = h_{V}(\bold{f})q = h_{Q}(\bold{f}) , k = h_{K}(\bold{f}) , v = h_{V}(\bold{f})$

本文中 query $qiq_i q_i$ 是点i的输入，表示几何位置 $xix_i$ . 因此如果有n个电，就有n个可能query。

**排列等变性：**

**一个关键自注意力机制关键的性质就是排列等变性。点1-n的排列影响自注意力机制结果的排列。这保证了自注意力机制的结果不任意依赖输入点的顺序。** Wagstaff et al.等发现这种机制可以逼近所有的排列等变函数。 SE(3)注意力机制是注意力机制空间的特殊样例，继承了这种排列等变性。然而，这种限制了学习函数在平移和旋转等变空间中。

#### **2.2 图神经网络**

注意力随点云大小的二次方缩放，因此引入邻居信息有所帮助(?如何理解)。并不是当前点和每一个点都建立联系，而是仅仅联系当前点附近的邻居节点。邻居节点的集合可以自然的用图来表示。自注意力机制曾以 Intra,self, vertex(顶点) 或者graph-attention的方式引入到图里。这些方法被 [Wang et al](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.07971) 统一为non-local 神经网络。简化表达为：

$yi=1ζ({fj∈Ni})∑j∈Niω(fi,fj)h(fj)                y_{i} = \frac{1}{\zeta(\{f_{j}\in N_i \})}\sum_{j\in N_i} \omega(\bold{f}_{i},\bold{f}_{j})h(\bold{f}_{j})                y_{i} = \frac{1}{\zeta(\{f_{j}\in N_i \})}\sum_{j\in N_i} \omega(\bold{f}_{i},\bold{f}_{j})h(\bold{f}_{j})$

其中 $ω和h\omega 和h\omega 和h$ 是神经网络， $ζ\zeta$ 是所有邻居 $NiN_i$ 特征 $f\bold{f}$ 的求和标准化的函数。于注意力机制有相似的结构，可以任务在每个邻域内执行attention操作。但是non-local模块不能够显示的包含边的信息，最可能的方式是直接相加。

#### **2.3 等变性**

给定一系列的变化 $Tg:V→V,forg∈GT_g: V\rightarrow{V} ,for \quad g \in GT_g: V\rightarrow{V} ,for \quad g \in G$ ,G是抽象图的组合，一个函数 $ϕ:V→Y\phi: V\rightarrow Y$ . 如果对于每一个g都存在一个变化 $Sg:Y→YS_g: Y\rightarrow Y$ 称函数 $ϕ\phi$ 是等变的。

$Sg[ϕ(v)]=ϕ(Tg[v])S_g[\phi(v)] =\phi(T_g[v])S_g[\phi(v)] =\phi(T_g[v])$

索引g可以被视为描述变化的参数。对于给定的pair $(Tg,Sg)(T_g,S_g)(T_g,S_g)$ 可以寻找上式满足等变性质的 $ϕ\phi$ 的家族。此外如果 $(Tg,Sg)(T_g,S_g)$ 是线性的，那么 $ϕ\phi$ 也是线性的。等变性文献中，深度网络通过交错线性映射 $ϕ\phi$ 和等变非线性来构建。 **tensor field network** 就是一种在3D旋转平移变化中合适的 $ϕ\phi$ 函数。

#### **Group Representations:**

一般来说，变化 $(Tg,Sg)(T_g,S_g)(T_g,S_g)$ 称为 **group representations**. 一个group representation $ρ:G→GL(N)\rho: G\rightarrow GL(N)$ .是一个group G 到一组 $N×NN \times N$ 的可逆矩阵 GL(N)的映射。严格来说， $ρ\rho$ 是group的同态。满足以下属性: $ρ(g1g2)=ρ(g1)ρ(g2)forallg1,g2∈G\rho(g_1g_2) = \rho(g_1)\rho(g_2) \quad for \quad all  \quad g1,g2 \in G$ . 具体的3D旋转 $G=SO(3)G= SO(3)$ ，而且有以下有趣的性质：

1. 它的表示是正交矩阵
2. 所有的标识都可以分解为： $ρ(g)=QT[⊕lDl(g)]Q\rho(g)  = Q^T[\oplus_{l}D_{l}(g)]Q\rho(g)  = Q^T[\oplus_{l}D_{l}(g)]Q$ . （5）

  

Q是正交矩阵，基变化矩阵。每一个 $DlD_lD_l$ ， $l=0,1,..,l=0,1,..,$ 是一个 $（2l+1）×(2l+1)（2l+1）\times(2l+1)$ 的矩阵（Wigner-D matrix）。 $⊕\oplus$ 是对角线元素直接求和或者拼接。

**[Tensor Field Networks](https://zhida.zhihu.com/search?content_id=234770982&content_type=Article&match_order=1&q=Tensor+Field+Networks&zhida_source=entity) ：**

**TFN** 是神经网络，在SE(3)等变的性质下，将3D点云旋转平移变化。对于点云来说输入是一个向量场 $f:R3→Rd\bold{f}: R^3\rightarrow R^d\bold{f}: R^3\rightarrow R^d$ .

$f(x)=∑j=1Nfjδ(x−xj)\bold{f}(\bold{x}) = \sum_{j=1}^N\bold{f}_j\delta(\bold{x}-\bold{x_j})\bold{f}(\bold{x}) = \sum_{j=1}^N\bold{f}_j\delta(\bold{x}-\bold{x_j})$

其中 $δ\delta\delta$ 是Dirac delta函数(单位脉冲函数)， ${xj}\{\bold{x_j}\}$ 是3D点云坐标， ${fj}\{\bold{f_j}\}$ 是点云特征，表示数量等例如原子序数或者点的编码信息。对于需要满足等变性， **TFN** 的特征需要满足等式（5）其中Q=I。每一个 $fj\bold{f_j}$ 都是一个多个不同类型types向量的拼接，其中一个子向量的type-l可以写为 $fjl\bold{f}_{j}^{l}$ 。 **TFN** 计算在空间中连续的卷积，可学习权重核 $Wlk:R3→R(2l+1)×(2k+1)W^{lk}:R^3\rightarrow R^{(2l+1)\times(2k+1)}$ 从type-k的特征到type-l的特征。type-l在 **TFN** 层 $xi\bold{x_i}$ 的输入为：

![](https://pic4.zhimg.com/v2-2075b44994dfb8c66d0de5a504ddd361_1440w.jpg)

$WlkW^{lk}W^{lk}$ 位于 ${WJlk}J=|k−l|k+l\{W^{lk}_J\}^{k+l}_{J=|k-l|}$ 的等变基尺度范围内。核是一组基础核的线性组合，其中第 $JthJ^{th}$ 系数是一个可学习的参数 $φJlk:R≥0→R\varphi_{J}^{lk}:R_{\geq 0}\rightarrow R$ （半径为 $||x||\left|| \bold{x} |\right|$ ）。数学形式为：

![](https://pica.zhimg.com/v2-8baab352c5936d757426de64a5746e94_1440w.jpg)

每一个基础核 $WJlk:R3→R(2l+1)×(2k+1)W^{lk}_{J} : R^3 \rightarrow R^{(2l+1)\times(2k+1)}W^{lk}_{J} : R^3 \rightarrow R^{(2l+1)\times(2k+1)}$ 是Clebsch-Gordan矩阵 $QJmlkQ_{Jm}^{lk}$ 的线性组合，其中J,m是线性组合的系数表示 第m维 的第J个 spherical harmonic（球谐函数） $YJ:R3→R2J+1Y_{J}: R^3\rightarrow R^{2J+1}$ 。 每一个基础核 $WJlkW_{J}^{lk}$ 在角度方向上完全约束可学习的核，静载径向方向上具有可学习的自由度。 $WJlk(0)≠0W_{J}^{lk}(\bold{0})\ne \bold{0}$ ，只有在k=l,同时J=0的情况下成立，同时将核简化为一个标量 $ω\omega$ 乘以对应的元素， $Wll=ωllIW^{ll} = \omega^{ll}\bold{I}$ ，定义为自相关。 **TFN** layer 可以写为：

![](https://pica.zhimg.com/v2-54facc189d9331a48e8acb8022ac61a8_1440w.jpg)

式子7和式子9 展示了信息传递的流程，信息从所有的节点和特征类型中聚合。也是non-local graph图操作的一种形式，权重是边的函数，特征 ${fi}\{\bold{f_i}\}\{\bold{f_i}\}$ 是节点的特征。将会看到提出的 **attention layer** 如何统一卷积以及图神经网络的各个方面。

![](https://pic4.zhimg.com/v2-ccc34e70ff24c9051a3af4551ed0c6f9_1440w.jpg)

Fig 2. 等变注意力机制下的节点特征更新

### 3\. 方法：

提出SE(3)-Transformer,可以分解为图2的几个步骤。下面介绍如何从点云构建一个图，如何构建图的等变边的函数，以及如何在图中传递SE(3）等变的信息，如何聚合这些信息。同时介绍一个姐替换的self-interaction layer， 称为 ***attentive self-interaction.***

**3.1 邻域：**

给定点云 ${(xi,fi)}\{(\bold{x_i},\bold{f_i})\}\{(\bold{x_i},\bold{f_i})\}$ , 定义其邻接节点 $Ni⊆{1,...,n}N_{i} \subseteq \{1,...,n\}$ ,以每个点i为中心.邻接节点通过最近邻算法以及指定的方法得到。例如分子的结构通过他们键结合的结构可以定义邻居。邻域的定义是的注意力机制的计算复杂度从点云数量的二次方降低到线性。

**3.2 SE(3)-Transformer**

**SE(3)-Transformer** 由三部分组成：

- 基于边的 attention 权重 $αij\alpha_{ij}\alpha_{ij}$ , 组成SE(3)不变的每条边 ij.
- 基于边的SE(3)-等变value 信息，在节点之间传递消息，像等式7中 **TFN** 卷积那样。
- 一个linear/attentive self-interaction 层，应用在每一个基础的邻域内：
![](https://pic3.zhimg.com/v2-1c42ecb378628be6f68e9807aef4e86c_1440w.jpg)

如果移除上式的attention 权重将会得到一个tensor field convolution 张量场的卷积. 如果移除应用在 $(xj−xi)(\bold{x}_j-\bold{x}_i)(\bold{x}_j-\bold{x}_i)$ $WVW_{V}$ ，那么将会得到传统的attention 机制。 在 attention 权重 $αij\alpha_{ij}$ 不变的调价下，上式10中对于SE(3)变化是等变的。因为这仅仅是对等变value massages的一种线性组合。不变attenion 机制可以通过标准化内积得到。对于一个节点i的query向量 $qiq_i$ , 以及邻域内每条边 edge\_ij 的邻域 $NiN_i$ 内 key 向量的集合 ${kij}j∈Ni\{ \bold{k_{ij}}\}_{j\in N_i}$ 有：

![](https://picx.zhimg.com/v2-5fb4ef30c463875b595c5b2f782896b7_1440w.jpg)

$⊕\oplus\oplus$ 是直接求和，本文中是向量拼接。 线性embedding 矩阵 $WQlkW_{Q}^{lk}$ 以及 $WKlk(xj−xi)W_{K}^{lk}(\bold{x_j}-\bold{x_i})$ 是 **TFN 类型。** attention 权重是不变的，输入的特征 ${fin,j}\{\bold{f_{in,j}}\}$ 是SO(3)等变的，同时 $qiq_i$ , ${kij}\{\bold{k_{ij}}\}$ 也是SE(3)等变的。由于线性embedding矩阵都是TFN 类型，那么内积也是等变向量，在相同的表示 $SgS_g$ 是不变的， $q→Sgq,k→Sgk,thenqTSgTSgk=qTkq\rightarrow S_gq, k\rightarrow S_gk, then \quad q^TS_g^TS_gk = q^Tk$ 。

**角度调制：**

**attention weights** 在TFN 核的基础上增加了角度方向的自由度，如等式10中卷积由 $αijWVlk(x)\alpha_{ij}W_{V}^{lk}(\bold{x})\alpha_{ij}W_{V}^{lk}(\bold{x})$ 核决定。有趣的是引入了attention weights的条件下 $αij\alpha_{ij}$ ，在保持等边性的基础上引入了对角度尺度 $WJlk(x/||x||)W_{J}^{lk}(\bold{x}/||\bold{x}||)$ .

**Channels, Self-interaction Layers, and Non-Linearities:**

类比传统的神经网络，SE(3)-Transformer可以直接拓展到每一个表示度l的多个通道。为self-interaction layer奠定了基础。attention laryer从节点中聚合信息，输入表示度为k. 对比来看，self-interaction 层单独在相同度的一个节点特征内交互信息。 类似cnn的1\*1卷积。self-interaction 是可学习跨越式关联，L层 中query 点i 到L+1 层query 点i 的一种优雅的信息传递方式。SE(3)-Transformer中每个节点并不关注自身。尝试了两种不同类型的self-interaction layer. 1)linear and 2) attentive.

$fout,i,c′l=∑cωi,c′cllfin,i,c.l\bold{f}_{out,i,c^{'}}^l =\sum_{c}\omega_{i,c^{'}c}^{ll}\bold{f_{in,i,c^{.}}^{l}}\bold{f}_{out,i,c^{'}}^l =\sum_{c}\omega_{i,c^{'}c}^{ll}\bold{f_{in,i,c^{.}}^{l}}$

**linear:** 输出通道的结果学习一个输入通道的线性组合，其中每个出入度的权重为 $ωi,c′cll=ωc′cll\omega_{i,c^{'}c}^{ll} = \omega_{c^{'}c}^{ll}\omega_{i,c^{'}c}^{ll} = \omega_{c^{'}c}^{ll}$ ,所有的节点共享。

**Attention:** 一种线性self-attention的拓展，attentive self-interaction. 由self-interaction 和 nonlinearity组成。将学习到的标量权重 $ωc′cll\omega_{c^{'}c}^{ll}\omega_{c^{'}c}^{ll}$ 替换为MLP输出的attenton的权重。所有的权重都是SE(3)不变的，在相同比表示的变化下，由于内积特征的不变性。

![](https://pic4.zhimg.com/v2-18361c64a13de46a40ab96412f2c708f_1440w.jpg)

**3.3 节点和边的特征：**

点云数据信息通常关联在节点特征以及边的特征上，将会作为网络第一层的输入。节点信息可以直接通过tensors $fj\bold{f_j}\bold{f_j}$ 来表示。对于边的信息， $fj\bold{f_j}$ 是多个邻域的组成部分。 $fj\bold{f_j}$ 可以用 $fijf_{ij}$ 来替代， $fijf_{ij}$ 在利用attention处理不同的邻域信息时，携带了不同的信息。换言之， $fijf_{ij}$ 携带了节点j以及边ij的信息。等价的，如果边的信息是标量，可以在径向网络中作为权重矩阵 $WV,WKW_V,W_K$ 输入。

### 4\. 实验：

三个数据集上做了实验，1. N体问题，预测粒子的位置和速度，输入的旋转对应引起输入结果的整体旋转。还有两个现实世界的分类任务。对比了SE(3)-Transformer和Set-Transformer,一个不等变的attention模型，以及没有attention的 **TFN** 网络；同时测量等变性的准确程度，利用统一的采样的SO(3)变化的输入和输出。不像Sosnovik et al等人，误差没有使用平方项：

![](https://pic3.zhimg.com/v2-34ff477d897bb8204f9149ffc0e4c190_1440w.jpg)

**4.1 N-Body Simulation:**

dataset from Kipf <sup><a href="https://zhuanlan.zhihu.com/p/#ref_2">[2]</a></sup>. 5个粒子每个可能带有正负电荷，相互之间施加排斥力或者吸引力。网络的输入是，一个粒子在某一个时间步的位置，速度，以及带电性。任务是预测未来500步之后的相对位置以及速度。将其定义为回归问题，避免单独预测多个时间步长。将等变注意力与例如LSTM结合等是一个有趣的研究方向。这里的目标验证与其他相关模型的核心贡献。这个任务与其他两个不同，因为它不是不变的而是等变的，当输入平移或者旋转时，对应的结果也会相应改变。

利用4个等变层训练SE(3)-Transformer, 每一个后面都接入了attentive self-interaction层，结果如下:

![](https://pica.zhimg.com/v2-4bdf7a5806e161cedfb11bd9365c08ce_1440w.jpg)

Table 1. N体预测结果

![](https://pic3.zhimg.com/v2-3660dab8e853dadc75f35558daddfb98_1440w.jpg)

Fig 3. 对比结果

**4.2 现实世界分类任务 基于ScanObjectNN：**

基于点云的目标分类任务是一个广泛研究的问题。有趣的是，需要主要的神经网络方法在研究标量坐标的时候没有考虑向量的归纳偏置。一些探索旋转不变性的点云分类方法逐渐提出。当前方法直接作用在点云上，无需预先将点云投影到球体上，这使得在信息传递过程中提高了采样的效率，类比于2D图片CNN方法的平移旋转不变性。为了验证当前方法，选择了 **ScanObjectNN，** 一个目标分类任务的数据集。基础评测提供了2902个目标的点云，包含了15个类别，仅仅利用电云作为输入，将类别作为预测的label。 利用4层 SE(3)-Transformer加线性self-interaction 后引入MLP的最大池化(max pooling)。有趣的是，该任务并不是完全旋转不变的，统计意义上来说，所有的对象沿重力轴对齐。这会导致使用完全的SO(3)不变模型会有性能上的损失。换句话说，当我们观察一个新对象的时候，知道朝上的方向在哪儿，对我们是有帮助的。又创建了一个SO(2)不变的算法版本，通过将z轴分量作为type-0的field，x,y 位置作用type-1的field，称这个版本为 SE(3)-Transformer+z.这样模型可以通过学习不同的促进和抑制输入来实现对称性（Fig 4a & Fig 4b）。Table 2 对比当前模型以及其他SOTA模型结果。

![](https://pic2.zhimg.com/v2-c953406ee392cd4cea6d084c19e5a59b_1440w.jpg)

![](https://pic4.zhimg.com/v2-493a39532a38f5b2b392e14f55ee4a59_1440w.jpg)

**4.3 QM9:**

**QM9** 是一个回归数据集，是一个公开的化学性质预测任务。 134k的分子，每个分子最多有29个原子。一个分子图中，原子被表示成一个5维的one-hot节点embedding, 有4种不同的化学键类型，每个原子有对应的位置坐标。6个回个任务，分值越低越好，表格结果被分为等变模型的和非等变模型的两类结果。

![](https://pica.zhimg.com/v2-c65978a7741858729eac31d77fea172e_1440w.jpg)

**5\. 结论：**

**提出了基于attention的针对点云数据的神经网络，能够保证输入的旋转和平移不变性结果鲁棒，无需增加训练时间保证了任意选择坐标系的稳定性。使用self-interaction 可以实现各向异性，数据自适应过滤，同时使用邻域可以实现对大型点云的操作。**

## 参考

1. SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks [https://browse.arxiv.org/pdf/2006.10503.pdf](https://browse.arxiv.org/pdf/2006.10503.pdf)
2. Neural Relational Inference for Interacting Systems dataset from Kipf et al [https://arxiv.org/abs/1802.04687](https://arxiv.org/abs/1802.04687)

还没有人送礼物，鼓励一下作者吧

编辑于 2023-10-12 14:11・北京[注意力](https://www.zhihu.com/topic/19569283)[Transformer](https://www.zhihu.com/topic/20746363)