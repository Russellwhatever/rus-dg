---
{"dg-publish":true,"permalink":"/02/2501/06/03/","noteIcon":"","created":"2025-04-28T20:23","updated":"2025-07-01T13:38"}
---

# 树莓派配置
## 1 基础功能
完成本地与树莓派端的依赖安装与摄像头配置后，可在使用浏览器接受信号或使用 python 文件接受信号，获得摄像头实时影像：
![99 Attachment/摄像头实时影像.gif](/img/user/99%20Attachment/%E6%91%84%E5%83%8F%E5%A4%B4%E5%AE%9E%E6%97%B6%E5%BD%B1%E5%83%8F.gif)
（此处对应文件夹中：`./摄像头实时影像.mp4` 或 `.摄像头实时影像.gif`）
## 2 视觉识别部分
类似第一阶段，创建控制小车运动的脚本，并通过 `REST API` 接收电脑传输的控制信号。
电脑本地则使用 YOLO 模型，对 8080 端口的视频进行实时检测，并依据物体（这里我尝试了常见的人、水杯、勺子等物体，其中后二者在由人手持时均易被判断为人的一部分，因此最终选择以人为识别物体）。
此时，已能够完成第二阶段的任务：小车识别摄像头前的人，并追逐人。
![99 Attachment/视觉识别.gif](/img/user/99%20Attachment/%E8%A7%86%E8%A7%89%E8%AF%86%E5%88%AB.gif)
- 站在小车正前方，小车向前（向人）移动；
- 靠近小车（蹲下），小车停转；
- 向左移动、向右移动至小车视野边缘，小车会向对应方向转向移动。
## 3 “趋利避害”小车
在此基础上，结合已有代码，对现有小车功能进行修改，使其不仅能够“追逐”某一物体，还能“避开”另一种物体（即看到该物体时后退；该物体从某侧进入视野即向另一侧转向）。
此外，添加了主要识别规则：当视野中存在不止一个物体时，根据其面积大小判断主次。
注意：为准确执行功能，此处的面积并不是所谓代码中方框的面积，而是 YOLO 模型识别出其物体的面积。如下图为例：
![99 Attachment/Pasted image 20250428224021.png](/img/user/99%20Attachment/Pasted%20image%2020250428224021.png)
修改过的 rasp-yolo.py 源代码已附在文件夹中，其主要功能执行方式：
1. 添加 avoid_categories，并在 send_command () 函数中增加判断，如果识别出物体为 avoid_categories 则切换模式为'avoid'，否则为'chase'。
2. 根据面积大小对识别出的物体进行排序，并将 avoid_categories target_categories 中面积最大的物体输入 send_command ()