---
{"dg-publish":true,"permalink":"/02/2501/06/05/","noteIcon":"","created":"2025-06-09T17:40","updated":"2025-07-01T13:38"}
---


<p align="right">杨皓翔</p>
<p align="right">22300220022</p>


# 1 概述
在2025年春季学期的《具身智能引论》课程中，我成功与小组成员通力合作，完成了小车基础功能的搭建，使其能够通过键盘操纵、图像识别、语音控制三种不同的方式对小车进行操纵，并添加了图像识别避开人类（详见3.2 视觉阶段扩展）、语音输入复杂序列指令（详见4.2语音识别阶段扩展）等功能。
# 2 小车基本功能
该阶段主要由小车本体的搭建、树莓派的基础配置与电脑端代码准备组成。
## 2.1 小车本体的搭建
小车本体由车板、电机、车轮与电池、树莓派、驱动组成。其中，电池为小车的驱动板供电，驱动板再通过一根USB转type-b线为树莓派供电。但实际操作中，为确保供电稳定，我在不需要大范围移动小车的情况下一般使用充电宝直接为树莓派供电。
四个车轮通过螺丝固定于车板的左右，并通过电线与驱动板相连；在实际运行过程中，车轮的电机会接受来自树莓派的GPIO接口的供电，从而向不同方向旋转，实现小车整体向不同方向的运动。在这一方面，我使用四种颜色的电线，以方便调整接口时方便区分和辨识；并将其修口，以避免铜线相互触碰，造成短路；在树莓派和驱动的GND添加一条便于拆装的杜邦线路，确保两个设备电压基准相同（如下图）。
车板上方，使用扎带固定塑料板以隔绝树莓派、驱动与车板。塑料板使用电磨笔进行开孔。
值得一提的是，在接线过程中，最初将树莓派上的GPIO接口接在了隔开GND接口的两端，导致两侧电压不同，并烧坏了一个树莓派。通过比较实验，我成功找出来导致树莓派烧坏的具体操作，并通过查找资料，确定了问题来源。在新树莓派上，通过严格控制接口号码与示例文档严格对应最终成功连接接口。

![99 Attachment/Pasted image 20250609234708.png](/img/user/99%20Attachment/Pasted%20image%2020250609234708.png)
## 2.2 树莓派配置
按照要求安装了对应版本的Ubuntu系统，并在首次启动之前设置了多个网络。在此之后顺利完成了环境配置和ros安装，并实现了使用本地PC的键盘对小车运动控制。
# 3 视觉控制阶段
## 3.1 基础流程
视觉阶段，小车的基本控制流程是：
1. 小车内部，通过摄像头获取图像信号
2. 树莓派通过mjpg-streamer将图像信号传递到本地PC上
3. 本地PC使用You Only Look Once(YOLO)对图像进行实时识别，并通过对所识别到的物体的位置和形状判断，输出不同指令
4. 保持接收信号状态的树莓派接收到指令，控制电机，让小车进行不同动作。
本阶段，最核心的代码是本地PC上，调用YOLO模型，并传出命令到树莓派的具体功能实现。当目标物体面积大于设定值时，认为已经接近物体，则停下；否则会根据其中点位置，进行向左和向右的转向。而`'command': "STOP"`等指令的具体定义则已在第一阶段定义于树莓派的ros包中。
```python
        if box_area > 0.7:  # 如果已经接近目标物体
            response = requests.post(control_url, json={'command': "STOP"})
            print("Stop")
        else:
            if x_center < 0.3:  # 左转
                response = requests.post(control_url, json={'command': "LEFT"})
                print("Turn left")
            elif x_center > 0.7:  # 右转
                response = requests.post(control_url, json={'command': "RIGHT"})
                print("Turn right")
            else:  # 前进
                response = requests.post(control_url, json={'command': "FORWARD"})
                print("Go straight")
```
## 3.2 拓展功能：多个物体的图像识别
本阶段，我按照文档，对原理代码进行详细研究。在顺利完成基础功能后，还对rasp_yolo.py进行简单修改，使其可以对预先设定的两种不同物体进行识别，并设定不同逻辑（在实际过程中，我设定为了“追逐”和“避让”）；当同时识别到两种物体时，会根据其实际面积（并非识别矩形框的面积）进行比较，以面积较大的物体代表的逻辑为准。
```python
def send_command(box, h, w, mode="chase"):
    x_center = (box[0] + box[2]) / 2 / w                          # 检测框的中心点
    box_area = (box[2] - box[0]) * (box[3] - box[1]) / (h * w)    # 检测框的面积
    print(f"box area: {box_area}, x_center: {x_center}, mode: {mode}")
    if mode == "chase":
        ...
    if mode == "avoid":
        ...
        
# 在进入识别后
while True:
    ret, frame = cap.read()  # 不停地从输入源读取一帧图像
    if not ret:  # 如果读取失败，结束循环
        break
    res_img = cv2.resize(frame, (cfg["width"], cfg["height"]),         interpolation=cv2.INTER_LINEAR)
    img = res_img.reshape(1, cfg["height"], cfg["width"], 3)
    img = torch.from_numpy(img.transpose(0, 3, 1, 2))
    img = img.to(device).float() / 255.0
    
    # 运行 YOLO 模型，得到输出
    preds = model(img)
    output = utils.utils.handel_preds(preds, cfg, device)
    output_boxes = utils.utils.non_max_suppression(output, conf_thres=0.3, iou_thres=0.4)

    # 加载 label names
    LABEL_NAMES = []
    with open(cfg["names"], 'r') as f:
        for line in f.readlines():
            LABEL_NAMES.append(line.strip())
            
    h, w, _ = frame.shape
    scale_h, scale_w = h / cfg["height"], w / cfg["width"]

    # 如果没有检测到任何物体，停止
    if len(output_boxes[0]) == 0:
        response = requests.post(control_url, json={'command': "STOP"})
        print("Stop")

    # 根据面积大小排序物体，优先处理面积最大的物体
    detected_objects = []
    for box in output_boxes[0]:
        box = box.tolist()
        obj_score = box[4]
        category = LABEL_NAMES[int(box[5])]
        x1, y1 = int(box[0] * scale_w), int(box[1] * scale_h)
        x2, y2 = int(box[2] * scale_w), int(box[3] * scale_h)
        area = (x2 - x1) * (y2 - y1)  # 计算物体的实际面积
        detected_objects.append((category, area, box))

    # 按面积从大到小排序
    detected_objects.sort(key=lambda x: x[1], reverse=True)

    # 处理检测到的物体
    for category, _, box in detected_objects:
        if category in target_categories:  # 如果是需要追逐的物体
            send_command(box, cfg["height"], cfg["width"], mode="chase")
            break  # 只处理面积最大的目标物体
        elif category in avoid_categories:  # 如果是需要避开的物体
            send_command(box, cfg["height"], cfg["width"], mode="avoid")
            break  # 只处理面积最大的目标物体
```
在最初设计时，我设定了当有多个同种物体时，以它们的面积总和进行比较（即，假如设定小车会追逐“人”，而避开“狗”，则如果画面中有三人一狗，且人的单独面积都小于狗，但总和面积小于狗，则小车将进行追逐）。但实际测试中发现，这样会需要在判断出“追逐”后，再添加一层逻辑进行判断，识别面积最大的人，从而导致识别过程的臃肿，小车控制逻辑变卡。此外，考虑到在实际操作中，会存在如广场这样无关路人较多的场景，因远处“无关路人”较多而妨碍近处“核心物体”的判断。因此，为简单起见，本算法会直接对识别出的物体按照面积排序，如果面积最高的那个物体是人，则追逐；是狗，则相反，简化了判断流程。
# 4 语音控制阶段
## 4.1 基础功能
语音控制阶段的主要实现逻辑与图像识别阶段相近，不过由于设定了语音信号由电脑的话筒输入，简化了小车识别语音、输出给本地PC的过程：
1. 人说话，电脑接受音频信号
2. 电脑调用Whisper模型，对音频信号进行识别
3. 当识别到预先设定的关键字时，电脑发出指令
4. 保持接受指令状态的树莓派接受指令，控制小车运动
## 4.2 输入系列复杂指令
与队友（尹培杨）合作，在简单的语音控制功能之上进行优化，实现了语音一次性输入一系列指令，再由树莓派依次执行的功能，并可灵活设置指令之间的间隔。
本地使用语音识别模型Paraformer，将有效语音片段转化为文本后逐字解析为指令列表，再发送对应的动作指令和执行时间。
```python
# !DIY.py
for char in text:
    if char in target_chars:
        parse_text.append(char)

...

if parse_text[i] in directions:
    direction = directions[parse_text[i]]
    if i + 1 < len(parse_text) and parse_text[i+1] in num_trans:
        number = num_trans[parse_text[i+1]]
        command_list.append((direction, number))
        
response = requests.post(control_url, json={'command': command, 'timeuse': timeuse})
```
而树莓派则接收`command`和`timeuse`两个参数，并调用control_motors执行
```python
# !rasp_control.py 

@app.route('/control', methods=['POST'])
def control():
    command = request.json.get('command')
    timeuse = request.json.get('timeuse')
    test_node.control_motors(command, timeuse)
    
...

sleep(timeuse)
for wheel, _ in self.motors_target_speed.items():
    self.motors[wheel].stop()
```
详可见附代码文件。
# 5 总结
经过本学期的课程实践，我在各位老师对具身智能原理的广泛传授、各位助教老师的切实指导、队友的彼此合作下，成功完成了小车从零件到成体、代码从无到有再到熟练掌握和修改的过程，无论是动手能力和代码实践经验都获得了长足的长进；视觉阶段和语音阶段分别完成了两项自主改进，对自身的创造力和成就感也得到了充分的满足，一时回首，居然收获颇丰。