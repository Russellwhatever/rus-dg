---
{"dg-publish":true,"permalink":"/02/2501/06/02/","noteIcon":"","created":"2025-04-20T19:07","updated":"2025-07-01T13:38"}
---


<p align="right">杨皓翔
学号：22300220022</p>

# 1 基本代码逻辑
## 1.1 建模
首先设置
```python
workspace = [
    [0, 5, 3, 0],
    [4, -2, 1, -3],
    [0, -4, 6, 5],
    [3, -2, -1, 20]
]
```
对问题进行建模，并设置终点处分数为 20 以进行奖励。
对于移动过程，通过 `get_legal_actions(state)` 首先确定有哪些地方是可以行走的（不可走出边缘，不可回到上一步的地方）
```python
def get_legal_actions(state):

    """获取当前状态下合法的动"""

    x, y, px, py = state

    legal_actions = []

    for a in range(4):

        dx, dy = directions[a]

        nx, ny = x + dx, y + dy

        if 0 <= nx < 4 and 0 <= ny < 4 and (nx, ny) != (px, py):

            legal_actions.append(a)

    return legal_actions
```
再使用贪婪算法（每一步选取最大的 Q，如果有多个相同最大的就随机选取）定义行走逻辑即可。
## 1.2 训练模型
使用不同的模型进行训练。其主要不同在于 Q 的更新方法（详见代码）：
Q-learning 是单一表征，直接使用 Q 代表下一步动作的 Q 值；double Q-learning 则使用独立的 $Q_{1}$ 和 $Q_{2}$，避免过高的估计偏差；Sarsa（一种 TD 方法）则是根据下一步的 Q 进行判断更新 Q 的方向。
# 2 训练结果
![99 Attachment/Pasted image 20250420191612.png](/img/user/99%20Attachment/Pasted%20image%2020250420191612.png)
分别使用三种强化学习训练方法进行寻路，得到了相同的结果，即：
![99 Attachment/Pasted image 20250420192928.png](/img/user/99%20Attachment/Pasted%20image%2020250420192928.png)
共得分 24 分（其中 20 分是我为终点设置的额外积分，因此实际应为 4 分）
# 3 附加题
改变两个条件：
1. 每一步只减少 4 分
只需修改每个模型算法中计算奖励 `r` 的部分：
```python
r = workspace[nx][ny] - 4 + (20 if (nx, ny) == (3, 3) else 0)
```
2. 可以回到上一步
只需修改 `get_legal_actions(state)` 函数
```python
def get_legal_actions(state):
    x, y, px, py = state
    legal_actions = []
    for a in range(4):
        dx, dy = directions[a]
        nx, ny = x + dx, y + dy
        # 修改前
        # if 0 <= nx < 4 and 0 <= ny < 4 and (nx, ny) != (px, py): 
        # 修改后
        if 0 <= nx < 4 and 0 <= ny < 4:
            legal_actions.append(a)
    return legal_actions
```
![99 Attachment/Pasted image 20250420200029.png](/img/user/99%20Attachment/Pasted%20image%2020250420200029.png)
修改后，最佳路径与之前一致，不过得分有所增加，变为 16 分（即上图中的 36 分减去我为终点设置的奖励 20 分）。