---
{"dg-publish":true,"permalink":"/02/2501/03/12-2/","noteIcon":"","created":"2025-05-15T23:50","updated":"2025-07-01T13:38"}
---

<p align="right">杨皓翔</p>
<p align="right">22300220022</p>
<p align="right">2025-05-15</p>

选择了Assault游戏：
![99 Attachment/Pasted image 20250516123501.png](/img/user/99%20Attachment/Pasted%20image%2020250516123501.png)
一款与SpaceInvader有些类似的射击游戏，并分别采用PPO和A2C模型的多层感知机策略，对多个同时运行的环境进行测试，并限制了单回合最大步数为$10^{7}$以防止出现无限循环。
```python
vec_env = make_vec_env("CartPole-v1", n_envs=4, env_kwargs={"max_episode_steps": 1000})
model = PPO("MlpPolicy", vec_env, verbose=1) # 创建 PPO 模型
model = A2C("MlpPolicy", vec_env, verbose=1) # 创建 A2C 模型
```
训练时间步均为100000。
对两种模型的测试结果均为`info:{'lives': 4, 'episode_frame_number': 0, 'frame_number': 0}`，经本地与egpu服务器环境重复实验，并可视化（`renderer_mode = 'human'`）观察，原因为测试的时间步太少，在角色掉血之前就已经结束。因此统一调高测试时间步为$10^{7}$。
三次重复PPO模型测试结果reward为189.0，315.0，168.0，均值为224.0；
三次重复A2C模型测试结果reward为210.0，189.0，189.0，均值为196.0，略逊于PPO模型