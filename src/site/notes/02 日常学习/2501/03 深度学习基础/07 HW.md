---
{"dg-publish":true,"permalink":"/02/2501/03/07-hw/","noteIcon":"","created":"2025-04-06T20:26","updated":"2025-07-01T13:38"}
---

<p align="right">杨皓翔</p>
<p align="right">22300220022</p>
<p align="right">2025-03-28</p>

# 1 修改时间步
分别设置时间步为120（5天）、24（1天）、48（2天）、100，对预测效果进行比较
## 1.1 120（5天）
![99 Attachment/Pasted image 20250406211750.png](/img/user/99%20Attachment/Pasted%20image%2020250406211750.png)
最佳 MAE：2.320
## 1.2 24（1天）
![99 Attachment/Pasted image 20250406212417.png](/img/user/99%20Attachment/Pasted%20image%2020250406212417.png)
最佳 MAE：2.323
## 1.3 48（两天）
![99 Attachment/Pasted image 20250406213657.png](/img/user/99%20Attachment/Pasted%20image%2020250406213657.png)
最佳 MAE：2.320
# 2 100
![99 Attachment/Pasted image 20250406214417.png](/img/user/99%20Attachment/Pasted%20image%2020250406214417.png)
最佳 MAE：2.323
******
以上可见修改时间步对训练结果精度和收敛速度的影响并不大
# 3 修改 RNN 隐藏层为50
## 3.1 原隐藏层=16
```python
# 超参数

input_size = raw_data.shape[-1] # 特征数=14

hidden_size = 16

output_size = 1
```
结果同第一部分时间步=120部分，最佳 MAE=2.320. 训练用时4min59s
![99 Attachment/Pasted image 20250406211750.png](/img/user/99%20Attachment/Pasted%20image%2020250406211750.png)
## 3.2 隐藏层=50
![99 Attachment/Pasted image 20250406215255.png](/img/user/99%20Attachment/Pasted%20image%2020250406215255.png)
最佳 MAE=2.301，训练用时4min56s，与原先变化不大。但训练过程中，MAE 与 Loss 的降速均明显低于隐藏层=16的模型，且在 epoch=10训练完成时，MAE 与 Loss 均低于隐藏层=16的模型。
# 4 修改 RNN 的激活函数为 ReLU
```python
# 定义RNN模型

class RNNModel(nn.Module):

    def __init__(self, input_size, hidden_size, output_size):

        super(RNNModel, self).__init__()

        self.rnn = nn.RNN(input_size, hidden_size,

                          batch_first=True, nonlinearity='relu'  ) # batch_size在第1维，即[batch, seq, feature]

        self.fc = nn.Linear(hidden_size, output_size)

  

    def forward(self, x):

        # hn：最后一个时间步的隐藏状态

        _, hn = self.rnn(x)

        # 仅返回最后一层的隐藏状态！

        out = self.fc(hn[-1])

        return out
```
训练时间4min58s，和修改前变化不大。
最佳 MAE=2.362，高于之前。
![99 Attachment/Pasted image 20250406220655.png](/img/user/99%20Attachment/Pasted%20image%2020250406220655.png)
相比 tanh，使用 ReLU 函数为激活函数，起始时误差更大，训练降速更快。
# 5 修改双层 LSTM 模型层数
## 5.1 原有双层模型
![99 Attachment/Pasted image 20250406221826.png](/img/user/99%20Attachment/Pasted%20image%2020250406221826.png)
最佳 MAE=2.341，训练用时5min11s
## 改为四层
最佳 MAE=2.334，训练用时6min28s，较两层 LSTM 有所提升。
![99 Attachment/Pasted image 20250407002944.png](/img/user/99%20Attachment/Pasted%20image%2020250407002944.png)
然而预测能力并没有较为明显的提升
