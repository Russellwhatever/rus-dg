---
{"dg-publish":true,"permalink":"/02/2501/03/11-1/","noteIcon":"","created":"2025-05-09T10:37","updated":"2025-07-01T13:38"}
---

# 1 Cartpole DQN训练
## 1.1 小体量测试
为节约时间，将代码中的500,000步减为5,000步，并用于测试。其训练结果如下：
移动轨迹：
```python
Left
Left
Left
Left
Left
Left
Right
Left
Right
Left
```
得分：10.0
## 1.2 现提高训练次数
设置学习时间为50,000步，（由于移动轨迹太长，略过），得分为263.0。可以观察到其中存在大量较长时间的左右重复，可以认为小车已经通过强化学习学会了反复左右移动以保持平衡。
# 2 冰湖Q-learning
尝试修改 greedy epsilon 的 decay 参数并与修改之前进行比较：分别设定 `decay = 0.005` 和 `decay = 0.0005`。测试结果中，未修改参数时 平均得分 1.0，标准差为 0；而增大 decay 十倍后，平均得分为 0，标准差为 0
# 3 太空入侵者
训练时间步 `total_timesteps=1e5`。由于游戏结果具有很大的随机性，反复测试三次，得分分别为：40，45，150，评价得分 81.7
改变训练时间步为 `total_timesteps=1e7`，测试三次得分分别为210，420，315，评价得分均值为315，可见对于要求复杂、动态行为的太空入侵者游戏，对训练时间步要求较高，提高训练步数具有显著的提升效果。
