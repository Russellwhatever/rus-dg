---
{"dg-publish":true,"permalink":"/02/2501/03/03/","noteIcon":"","created":"2025-03-06T20:01","updated":"2025-07-01T13:38"}
---

# 1 前馈神经网络
线性变换与非线性变换
- 感知器/单层神经网络：只能解决线性问题
- 两层感知器：可以解决异或问题
- 前馈神经网络：大规模的并行分布式处理器，天然具有存储和使用经验知识的能力。
> 内部神经元的连接强度，即突触权重，存储知识
> 当隐含层神经元的数量足够多，神经网路能够以任意精度去逼近一个给定的**连续**函数。但没有说明如何找到这样的网络，以及该网络是否最优

# 2 pytorch（参看文件pytorch.ipynb）
## 2.1 准备数据
#AI人工智能/数据准备/StandardScaler 
`StandardScaler()`的作用
1. 加速梯度下降算法的收敛速度
2. 防止计算过程中数值不稳定
3. 提高模型性能，尤其是基于距离的算法和基于梯度的算法
4. 统一尺度，每个特征对模型的贡献更加均衡
![99 Attachment/Pasted image 20250306201324.png](/img/user/99%20Attachment/Pasted%20image%2020250306201324.png)
### 2.1.1 K-fold交叉验证法
优点：
1. 充分利用有限的数据
2. 模型性能估计稳定，避免单一训练/训练集带来的偏差
![99 Attachment/Pasted image 20250306201624.png](/img/user/99%20Attachment/Pasted%20image%2020250306201624.png)
## 2.2 留一法：一种特殊的交叉验证方法
优点（除了以上提到的）：
1. 对小数据集特别有用
缺点：
1. 计算成本高，需要计算 n 次模型
2. 对于大数据集可能会特别耗时
- [x] 问题：与 K-fold 交叉验证方法有什么区别 [completion:: 2025-05-18]
![99 Attachment/Pasted image 20250306201919.png](/img/user/99%20Attachment/Pasted%20image%2020250306201919.png)
## 2.3 自助法
重复地取出。所以数据集中可能有重复样本，同时有样品没有利用上
![99 Attachment/Pasted image 20250306202744.png](/img/user/99%20Attachment/Pasted%20image%2020250306202744.png)
# 3 模型搭建
## 3.1 前馈神经网络
### 3.1.1 序列容器
按照传入顺序添加到容器中:
![99 Attachment/Pasted image 20250306203226.png](/img/user/99%20Attachment/Pasted%20image%2020250306203226.png)
```python
# 利用索引查询模型中的第一层、第二层
print(model[0])
print(modle[1])

# Linear(in_feature=4, ouut_feature=10, bias=True)
# ReLU()
```
## 3.2 常用激活函数
#AI人工智能/激活函数 
> 作用：引入非线性，让网络能够逼近任意复杂的函数，解决线性模型无法解决地问题。将输出值归一化，有助于训练过程的稳定。

![99 Attachment/Pasted image 20250306203530.png](/img/user/99%20Attachment/Pasted%20image%2020250306203530.png)
## 3.3 损失函数
```python
Loss = nn.CrossEntropyLoss() # 交叉熵
```
常用的损失函数：
![99 Attachment/Pasted image 20250306203637.png](/img/user/99%20Attachment/Pasted%20image%2020250306203637.png)
```python
# BCELoss() 二元交叉熵损失
target = torch.tensor([[0.0, 1.0], [1.0, 0.0]])
output = torch.tensor([[0.2, 0.9], [0.6, 0.9]])
criterion = torch.nn.BCELoss()
loss = criterion(output, target)
print(loss)
```
$BCE=\sum^{N}_{i=1}(-\hat{y_{i}}\log_{i}\hat{y_{i}}-(1-\hat{y_{i}})\log(1-\hat{y_{i}}))$
## 3.4 优化器
```python
optimizer = torch.optim.Adam(
        model/parameters(), # 待优化参数
        lr = 0.01) # 学习率
```
![99 Attachment/Pasted image 20250306204724.png](/img/user/99%20Attachment/Pasted%20image%2020250306204724.png)
### 3.4.1 批量梯度下降、随机梯度下降
![99 Attachment/Pasted image 20250306204801.png](/img/user/99%20Attachment/Pasted%20image%2020250306204801.png)
# 4 训练模型
## 4.1 自定义计算准确率函数
![99 Attachment/Pasted image 20250306205214.png](/img/user/99%20Attachment/Pasted%20image%2020250306205214.png)
## 4.2 BP 算法（反向求导）
```python
x.backward() # 反向求导，计算x对所有输入的偏导数

print(a.grad)
print(b.grad)
```
Ppt 的 P55-57有数学的详细推导
### 4.2.1 结论
H 层权值调整公式
![99 Attachment/Pasted image 20250306210147.png](/img/user/99%20Attachment/Pasted%20image%2020250306210147.png)
## 梯度消失与梯度爆炸
激活函数导数：既不能小于1，也不能大于1
> 但是，直接 $y=x$ 又不能解决非线性问题
**因此**，有了 `ReLU()` 函数（Rectified Linear Unit 校正线性单元）
![99 Attachment/Pasted image 20250306210433.png](/img/user/99%20Attachment/Pasted%20image%2020250306210433.png)
![99 Attachment/Pasted image 20250306210527.png](/img/user/99%20Attachment/Pasted%20image%2020250306210527.png)

